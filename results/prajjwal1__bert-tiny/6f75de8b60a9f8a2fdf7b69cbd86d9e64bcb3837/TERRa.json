{
  "dataset_revision": "7b58f24536063837d644aab9a023c62199b2a612",
  "evaluation_time": 0.33466053009033203,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.47",
  "scores": {
    "dev": [
      {
        "cosine": {
          "accuracy": 0.5276872964169381,
          "accuracy_threshold": 0.9695652723312378,
          "ap": 0.4824715803060732,
          "f1": 0.6623093681917211,
          "f1_threshold": 0.9293473362922668,
          "precision": 0.49673202614379086,
          "recall": 0.9934640522875817
        },
        "dot": {
          "accuracy": 0.511400651465798,
          "accuracy_threshold": 121.7265625,
          "ap": 0.46884354944768114,
          "f1": 0.663755458515284,
          "f1_threshold": 112.94850158691406,
          "precision": 0.49836065573770494,
          "recall": 0.9934640522875817
        },
        "euclidean": {
          "accuracy": 0.5276872964169381,
          "accuracy_threshold": 2.718472957611084,
          "ap": 0.48258230070572494,
          "f1": 0.6623093681917211,
          "f1_threshold": 4.120393753051758,
          "precision": 0.49673202614379086,
          "recall": 0.9934640522875817
        },
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.48258230070572494,
        "manhattan": {
          "accuracy": 0.5244299674267101,
          "accuracy_threshold": 24.713253021240234,
          "ap": 0.4818495984805969,
          "f1": 0.6623093681917211,
          "f1_threshold": 38.33557891845703,
          "precision": 0.49673202614379086,
          "recall": 0.9934640522875817
        },
        "max": {
          "accuracy": 0.5276872964169381,
          "ap": 0.48258230070572494,
          "f1": 0.663755458515284
        },
        "similarity": {
          "accuracy": 0.5276872964169381,
          "accuracy_threshold": 0.9695651531219482,
          "ap": 0.4824715803060732,
          "f1": 0.6623093681917211,
          "f1_threshold": 0.9293473958969116,
          "precision": 0.49673202614379086,
          "recall": 0.9934640522875817
        }
      }
    ]
  },
  "task_name": "TERRa"
}