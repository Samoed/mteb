{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 3.6090335845947266,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.47",
  "scores": {
    "test": [
      {
        "accuracy": 0.523291015625,
        "ap": 0.5132463334817466,
        "ap_weighted": 0.5132463334817466,
        "f1": 0.5183761535455147,
        "f1_weighted": 0.5183761535455147,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.523291015625,
        "scores_per_experiment": [
          {
            "accuracy": 0.54052734375,
            "ap": 0.5223122525883069,
            "ap_weighted": 0.5223122525883069,
            "f1": 0.535968235737444,
            "f1_weighted": 0.535968235737444
          },
          {
            "accuracy": 0.56494140625,
            "ap": 0.5373067652925532,
            "ap_weighted": 0.5373067652925532,
            "f1": 0.5631540505077274,
            "f1_weighted": 0.5631540505077274
          },
          {
            "accuracy": 0.486328125,
            "ap": 0.4933539496527778,
            "ap_weighted": 0.4933539496527778,
            "f1": 0.4862967710431545,
            "f1_weighted": 0.4862967710431545
          },
          {
            "accuracy": 0.51611328125,
            "ap": 0.5084264168984701,
            "ap_weighted": 0.5084264168984701,
            "f1": 0.5051377875048246,
            "f1_weighted": 0.5051377875048246
          },
          {
            "accuracy": 0.54833984375,
            "ap": 0.5258585755557517,
            "ap_weighted": 0.5258585755557517,
            "f1": 0.5310722785842561,
            "f1_weighted": 0.5310722785842561
          },
          {
            "accuracy": 0.5068359375,
            "ap": 0.5034610782657658,
            "ap_weighted": 0.5034610782657658,
            "f1": 0.5059647835586547,
            "f1_weighted": 0.5059647835586547
          },
          {
            "accuracy": 0.57470703125,
            "ap": 0.542760410507332,
            "ap_weighted": 0.542760410507332,
            "f1": 0.5745965804281441,
            "f1_weighted": 0.5745965804281441
          },
          {
            "accuracy": 0.48876953125,
            "ap": 0.49450379824308754,
            "ap_weighted": 0.49450379824308754,
            "f1": 0.48831558759246624,
            "f1_weighted": 0.48831558759246624
          },
          {
            "accuracy": 0.52587890625,
            "ap": 0.5137091400112234,
            "ap_weighted": 0.5137091400112234,
            "f1": 0.5238708858728899,
            "f1_weighted": 0.5238708858728899
          },
          {
            "accuracy": 0.48046875,
            "ap": 0.49077094780219777,
            "ap_weighted": 0.49077094780219777,
            "f1": 0.46938457462558636,
            "f1_weighted": 0.46938457462558636
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}